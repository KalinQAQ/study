2022 年底，OpenAI 推出 ChatGPT，引发了大语言 AI 热潮。人们惊讶于其强大的理解能力和内容生成能力，各行各业都在思考如何用 AI 为企业赋能，AI 应用的落地成了大家都在追寻的“One Piece”。

当然，对于普通打工人，AI 的强大也会让我们感到危机，以前 1 天干的活，现在 AI 一分钟就给你干完了，而且干得比你还好。生活已经够难了，AI 还来抢“饭碗”。。。

对于开发者， 更多的却是机遇，我们可以利用大语言模型的强大，开发各类此前无法想象的 `AI+ 应用`。现在，请大家快速登船，我们的 AI 未来号，启航了！

本节将会从大语言模型`发展史`、`使用场景`以及`相关基础知识`这几个方面进行讲解，为便于你更好地理解和记忆，我也整理出了一份思维导图，如下所示：

![image.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1151f8d7a6b64b3ea8ec2acae5045d7e~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=1280&h=924&s=181246&e=png&b=ffffff)

## 大语言模型发展史

现在，我们先来回顾下大语言模型的前世与今生，看看每个阶段是如何为更高级的模型打下基础。其发展历程可以追溯到 20 世纪中期……

**第一阶段**，**HMM**

隐马尔可夫模型（HMM）于 20 世纪中期首次出现，并在 20 世纪 70 年代开始流行，该模型对句子的语法结构进行编码，用于预测新单词。HMM 在预测新内容时，**只会考虑最后的输入**。比如：输入“I went to the store and ”，需要预测生成新内容，但由于 HMM 只会看到最后一个标记“and”，信息如此少，不太可能给到我们满意的预测。

**第二阶段**，**N-gram**

20 世纪 90 年代，N-gram 模型开始流行起来，与 HMM 不同，**N-gram 能够接受一些标记作为输入**。比如，对于前面的例子，N-gram 能根据 “the store and” 来预测后面的内容，所以会表现得比较好。但**由于接收的输入标记数量还是很有限，所以预测效果还是不理想**。

**第三阶段**，**RNN**

2000 年左右，循环神经网络变得特别流行，因为它们**能够接受更多数量的输入标记**。特别是，LSTM 和 GRU（RNN 的类型）得到了广泛的应用，并且可以产生相当好的结果。然而，**RNN 在处理很长的文本序列时存在不稳定问题**。

**第四阶段，** **Transformer**

2017 年，Google 发布了 Transformer 模型，Transformer 允许输入标记数量大幅增加，消除了 RNN 的不稳定问题。

同时，利用`注意力机制`，模型学会对输入的不同部分分配不同的注意力，更关注那些对当前任务更重要的信息，能够更好地捕捉上下文关系，输出更合理的内容。以上面例子举例，输入“I went to the store and ”，模型预测生成了“bought”，在这种情况下，模型除了需要预测 “buy” 这个动词外，还需要根据上下文中的“went”推断出过去式。

**第五阶段，** **GPT**

GPT 是基于 **`Transformer`的一种特定模型**，通过在大规模语料库上进行无监督的预训练，然后在特定任务上进行微调，可以适应各类任务场景，尤其在文本生成、对话系统、问题回答等生成式任务上表现出色。这个 GPT 也是我们本课程要学习的大语言模型。

## 大语言模型使用场景

GPT 经过了大量语料的预训练和特定任务的微调，在各行各业都能“有所作为”，与其问它能做什么，不如问想让它做什么。本小节，我们将列举一些典型的应用场景，带大家初步领略下大语言模型的潜力和魅力。

* **智能客服**。利用大语言模型强大的语义理解能力，加上检索增强生成（RAG）技术，可以很方便地训练出基于本地知识库的智能客服。可能以后淘宝小二一声声“亲”的背后，不再是抠脚大汉，而是人工智能了。

* **智能教育**。AI+ 教育，也是 AI 应用的一个大方向。想象一下，以后不用大成本，就能“请”到一个非常专业、非常全面的家庭教师，能根据学生的学习情况实时调整学习计划，提供个性化的教育辅导，帮助学生更好地理解和掌握知识，实现真正的教育平等。

* **编程辅助**。说到我们最熟悉的编程领域，在提效方面，只要用过 ChatGPT 的同学，肯定都深有体会。以前遇到不懂的语法、想不通的需求，我们都是先翻个半小时 Google，然后在众多不靠谱的资料中找答案。现在，跟 ChatGPT 聊着聊着就把活干完了，写代码跟说话一样简单。更有专门优化过的 AI 代码辅助工具，像 GitHub Copilot，能够分析程序员编写的代码、注释和上下文信息，自动生成高质量代码。

* **文本总结**。GPT 在文本总结方面也是一大利器，模型能够快速理解文章的主题和关键信息，提炼关键信息，然后生成简洁而准确的摘要。网上已经有很多相关的 AI 工具，比如 ChatPdf、ChatDoc，这些工具允许我们上传自己的文件后，可以对它提问任何关于这份上传文件的任何内容，AI 会帮我们进行总结和分析。

* **数据分析**。利用 GPT 的自然语言处理能力，我们可以更方便地解读和理解数据，做出决策，提高整个数据分析流程的效率。比如，在分析多个复杂图表时，以往我们需要耗时很长才能发现数据的趋势和变化，GPT 可以一眼分析出图表之间的关联性，并输出一份简单易懂的解读报告，帮助我们挖掘出有用信息。

  另外，将 GPT 与传统数据分析工具相结合，比如 GPT + SQL，数据分析人员不再需要写复杂难懂还容易出错的 sql 语句，只需要以自然语言形式提出问题，例如：“最近几个月哪个产品的销售额增长最快？” GPT 便能自动转成 sql 语句，去数据库中查询得到结果，再结合一些报表工具，还能一步到位画出图表。

## 大语言模型基本知识

我们已经了解了大语言模型的应用场景，在开始学习 AI 应用开发前，我们还需要学习一些大语言模型的基本知识。

### LLM、GPT、ChatGPT 是什么关系？

首先，我们要理清 LLM、GPT、ChatGPT 这三者的区别和联系。

* **LLM**，全称 Large Language Model，泛指大型语言模型，这其中不仅包括 GPT 和 ChatGPT，也包括其他大语言模型。我们本课程说的 AI 应用开发，也可以称 LLM 应用开发。
* **GPT**，全称 Generative Pre-trained Transformer，由 OpenAI 开发的一系列模型，基于 Transformer 架构，通过预训练学习大规模数据，然后在特定任务上进行微调，可以适应各种自然语言处理任务。
* **ChatGPT** 是 GPT 系列模型的一个特定应用，经过微调优化，使其更像人类，能与人类进行更有效的对话。

三者的关系如下图所示：

![image.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c1cf2d82ea894d398069cb967f41f7d1~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=1194&h=706&s=52415&e=png&b=ffffff)

### 提示词（Prompt）：与大语言模型对话的艺术

**提示词通俗讲就是输入给大语言模型的文字，是驱动大语言模型运行的命令**。

传统的编程程序，只有预期的输入才会给出输出，必要时候还会给出报错，我们能根据报错信息进行调整。

大语言模型不同，它就像一个人，输入的提示词如何，都会给出相应的回复。所以，提示词的选择对大语言模型的回复质量有显著影响。下面两个提示词例子：

1. “怎样找到一份好工作？”
2. “我是一名新进的生物技术研究生，我应该如何找到与我的专业相关的工作？可以提供一些求职网站和技巧吗？”

第一个提示词，模型能得到的信息很少，因此回答的质量肯定不如第二个。我们在选择提示词时，需要反复尝试和优化，给出具体、清晰的指令，才能引导模型生成想要的结果。在后续的课程中，我们会专门介绍一些常用的提示技巧，帮助大家更好地了解和学习如何写出高质量的提示词。

在 AI 应用开发中，提示词更多地用于限制和规范模型的输出结果，比如限制模型只能回答编程相关的问题、限制模型的结果以 markdown 形式返回，等等。

### 令牌（Token）：大语言模型基本的计量单位

在 GPT 中，以 token 为基本计量单位，GPT 的基本工作原理是：**输入 n 个 token，GPT 模型计算输出 1 个 token**。如下图所示：

![image.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e04fcac993354b59b430b30a887ce657~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=1280&h=390&s=49487&e=png&b=fefbfb)

Token 也是大语言模型的计费单位，输入 Token 和输出 Token 的计费价格不同，同系列模型不同版本之间的价格也不相同，比如 OpenAI 的 GPT-3.5，每输入 1000 个 Token，需要花费 0.0015 美元，每输出 1000 个 Token，需要 0.002 美元；GPT-4，由于性能更好，价格更贵，大概是 GPT-3.5 的 30 倍。

一个 token 不一定是一个完整的单词，可以是短单词，也可以是单词的一部分。GPT 会将文本按一定的算法拆成多个 token，以 Open AI GPT 模型举例，常见单词和短单词通常单独拆成一个 token，而一些不常见或者长度较长的单词会被拆成多个 token。

OpenAI 提供了一个 [Tokenizer 页面](https://platform.openai.com/tokenizer "https://platform.openai.com/tokenizer")，可以用来可视化显示文本的 token 拆分情况，如下图所示：

![image.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b93d594de6df4d76a6aa4c52660a0543~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=1280&h=470&s=41537&e=png&b=f7f7f8)

此外，还提供了一个 token 分词的 python 库：[github.com/openai/tikt…](https://github.com/openai/tiktoken "https://github.com/openai/tiktoken") 。

#### 扩展输出

上面我们提到，GPT 的基本原理是输入 n 个 token，输出 1 个 token。但玩过 ChatGPT 的同学都知道，输入一个问题后，ChatGPT 是会输出一串文本的，也就是说并不是输出一个 token。这个实现的基本思路是：**输入 n 个 input tokens，生成 1 个 output token，然后将该 output token 合并到 input tokens 中，作为下一次输入的一部分，迭代计算，以此类推，直到输出完整的文本**。

如下图所示：

![image.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/7055c63109a24af882f9e05ccef50da1~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=1280&h=402&s=173374&e=png&b=fcf1f1)

#### 输出随机性

使用 ChatGPT 时，我们还会发现，ChatGPT 的回答具有不确定性。输入两个完全一样的问题，得到的答案可能不一样。这是因为 GPT 每次迭代计算时，先生成一组带有不同概率的 token 列表，如下示意图，GPT 从其中选择一个 token 作为 output token。

![image.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0504292225d949fe8acd71107e79b49a~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=1174&h=490&s=53531&e=png&b=ffffff)

选用哪个 token 呢？正常理解肯定选择概率最大的 token。但经过多次实验发现，如果我们总是选择排名最高的 token，最后得到的文本会很“平淡”，不够“拟人化”。但如果有时选择一个排名比较低的 token，会有意想不到的效果。

所以，我们在选择 output token 的时候，**添加了一个 `temperature`的参数因子**。该参数取值 0～1，决定了使用低排名 token 的频率。实践表明，**`temperature` = 0.8 效果较好**。

由于 temperature 的引入，导致 GPT 的输出具有一定的随机性，所以相同的输入会得到不同的回答。

#### token 数量限制

大语言模型会限制单次的 token 数量，主要有以下几个原因：

* 模型的参数规模与 token 数量呈线性关系，处理 token 数量越多，计算开销越大。
* 模型的输入和生成的临时结果需要暂存在内存中，过长的文本会占用太多的内存。
* 随着输入文本的增长，模型推理的效率和准确性不断降低，甚至会报错。

因此，出于资源和效率的考虑，限制 token 数量是十分有必要的，能够确保模型在可接受的时间和资源范围内进行训练和推理。

不同 GPT 模型限制大小不同，GPT-3 限制 4096 个 token，GPT-4 限制 8192、32768 或 128000 个 token（GPT-4 本身也分了好几个版本）。

需要注意的是，该限制是“输入+输出”的总数量。比如使用 GPT3 时，如果输入 4090 个 token，那 GPT 只能输出 6 个 token 作为响应。所以，**我们在输入的时候，需要格外考虑输出的剩余可用 token 数**。

### 微调（Fine-tuning）：训练自己专属的大语言模型

此前我们说过，GPT 经过了大量语料的预训练，能够适应各类任务场景。但对于一些特定的场景，GPT 可能无法达到用户预期，比如某个细分领域甚至私域的提问，GPT 只能给出一些“正确的废话”。为了解决这个问题，我们可以用我们自己的训练数据对大语言模型进行微调，提高模型回答的质量和效果。

这就好比一个基础知识扎实的程序员，新入职了一家公司，对公司的开发流程、开发项目都不了解，你是他的导师，带他尽快熟悉整个工作流程。之后他结合自己的专业和技术，快速上手，甚至上位，升职加薪，而你，慢慢被淘汰了。。。

这种对新员工的入职培训，就是大语言模型中的微调。

GPT 的微调，不需要很高的门槛，直接通过调用 API 就可以完成。**我们只需要准备好所需的训练数据，发送给微调的 API，就可以获得一个专属的、强化后的 GPT 模型**。

其实，ChatGPT 时刻也在进行着微调，大家在使用时，有时候可能会遇到模型同时有两种不同的输出，让我们自己选择更喜欢哪个。ChatGPT 会将该选择反馈到模型中，不断优化，使输出更符合人类预期。这种微调技术称为`人类反馈强化学习（RLHF）`。

## 总结

最后，我们总结下本文的知识要点。

1. GPT 是基于 Transformer 的一种特定模型，通过在大规模语料库上进行无监督的预训练，然后在特定任务上进行微调，以满足各类任务场景。
2. 大语言模型在客服、教育、编程、数据分析等各个方向都能发挥作用。
3. 提示词的好坏会直接影响 GPT 的回复质量。
4. Token 是 GPT 的计量单位和计费单位；Token 的预测具有随机性；GPT 对单次输入和输出的 token 数量有限制。
5. 为了提高大语言模型在某个细分领域的回答质量，可以通过微调技术，低成本获取一个专属的大语言模型。

欢迎大家在评论区留下自己的想法和理解，我们可以进一步交流学习。