大家好，欢迎继续学习 LangChain 实战课程。

> 实战项目代码仓库地址：[github.com/huiwan-code…](https://github.com/huiwan-code/wx_bot "https://github.com/huiwan-code/wx_bot")

上节课，我们已经能够让公众号实现自动回复了，今天，我们接入 AI，让其能检索文章内容并进行作答。

既然是 RAG 应用，那肯定有构建知识库和检索回复两部分。我将项目分为**管理端和用户端**：管理端负责上传公众号文章并存储到向量数据库中，用于构建我们的知识库；用户端则负责接收用户问题，检索知识库并调用 LLM 进行响应。

# 管理端上传公众号文章

在管理端，我们做出的效果是：上传一个公众号文章链接，自动爬取该链接文章内容，并将内容存入数据库中。

前端页面很简单，一个输入框接收一个链接，再有一个提交按钮，点击后把链接提交到后端服务接口上。管理页面展示如下：

![image.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d45d488925314461a00d9ba3a49d5cf9~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=981&h=541&s=30832&e=png&b=f7f8f9)

前端代码大家可以直接去 [github](https://github.com/huiwan-code/wx_bot/blob/main/wx_bot/static/post_wechat_article.html "https://github.com/huiwan-code/wx_bot/blob/main/wx_bot/static/post_wechat_article.html") 查看。

我们重点聊聊后端的处理逻辑和细节。

我们定义了`WechatArticlePostResource`资源类，`def get(self)`方法用于返回上传微信文章的前端页面；`def post(self)`方法用于接收页面提交的文章链接，进行内容拉取并存储。

`post`方法获取到需要存储的文章链接后，第一步就是要去解析文章内容。 浏览器打开一篇微信文章，F12 打开【元素检查】，定位文章标题和文章内容

![image.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/46a7b99e8ec249c7a55be4cd0fe65e2d~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=1573&h=640&s=362823&e=png&b=f9f7f2)

![image.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6d312167bc9f40adbfeb966b21ae9afe~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=1566&h=613&s=333906&e=png&b=f1f5e5)

根据上面两个图可以看到，文章的标题位于`class = 'rich_media_title'`的`h1`标签下；文章内容位于`class = 'rich_media_content'`的 `div`标签下。

我们先使用 requests 库将链接的完整 html 内容下载下来，再使用 bs4 库对内容进行解析。我们将这段逻辑封装到`WechatArticlePostResource::parse_article`方法中，具体代码如下：

```python
class WechatArticlePostResource(BaseResource):
    def parse_article(self, article_url):
        from bs4 import BeautifulSoup
        # 模拟头部
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'
        }
        response = requests.get(article_url, headers=headers)
        # 文章获取失败
        if response.status_code != 200:
            return None
        
        soup = BeautifulSoup(response.text, 'html.parser')
        # 获取文章标题
        title = soup.find('h1', class_='rich_media_title').get_text(strip=True)
        # 获取文章内容
        content = soup.find('div', class_='rich_media_content').get_text()
        article_data = {
            'title': title,
            'content': content
        }
        return article_data
```

获取到文章内容后，在对其进行向量化入库前，我们还需要考虑是否需要对其进行切割。

由于我的文章是中文内容，嵌入模型采用`bge-large-zh`。

![image.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1b47213f448949a89275f43a6a5641dd~tplv-k3u1fbpfcp-jj-mark:1600:0:0:0:q75.jpg#?w=1145&h=596&s=135512&e=png&b=fefefe) [评分链接](https://cloud.baidu.com/qianfandev/model/49 "https://cloud.baidu.com/qianfandev/model/49")

该模型单次最多支持 512 个 token，而我文章字数平均在 2000字符左右，所以需要先对内容进行分割。

为了保证分割效果，采用的是`RecursiveCharacterTextSplitter`，分割字符为`['。','，', '']`，最后的`''`确保了分割后的大小肯定符合预期。

> 忘了`RecursiveCharacterTextSplitter`效果的读者可以回顾《[文档分割](https://juejin.cn/book/7352849785881591823/section/7353085944972443698 "https://juejin.cn/book/7352849785881591823/section/7353085944972443698")》这一小节

```python
splitter = RecursiveCharacterTextSplitter(
            chunk_size=700,
            chunk_overlap=300,
            separators=['。','，', '']
        )
docs = splitter.create_documents([article_data['content']], [{'title': article_data['title'], 'source': article_url}])
```

我们限制分割后的文档小于等于700字符，实测下来，512token是能涵盖住700个字符的。

使用`splitter.create_documentsd`创建最终的文档集，并将文章链接和文章标题作为每个文档的元信息(metadata)。

接下来，我们需要将文档集存入向量数据库，方便起见，我们还是选用chroma。相关逻辑封装在`store_doc`方法中。

```python
def store_docs(self, docs: list[Document]):
        # page_content+source生成一个doc_id，避免重复存储
        ids = []
        for doc in docs:
            doc_id = hashlib.md5(str(doc.page_content + doc.metadata['source']).encode('utf-8')).hexdigest()
            ids.append(doc_id)
        
        # 存入chroma向量数据库
        Chroma.from_documents(
            documents=docs,
            embedding=get_embeddings_endpoint(), 
            persist_directory=config.DB_STORE_DIR,
            ids=ids
        )
```

为了避免重复上传一篇文章导致数据库内容重复，我们对文档内容和文档来源（链接）进行md5哈希得到文档的id。传给chroma,用于落库时对文档去重。

> 文档去重相关知识可见：[数据嵌入与向量存储](https://juejin.cn/book/7352849785881591823/section/7353248115098386458#heading-8 "https://juejin.cn/book/7352849785881591823/section/7353248115098386458#heading-8")

# 用户端回复用户

## 什么时候需要回复？

大部分公众号都会设置一些关键字，用于触发某些自动回复。比如用户发送特定字符，公众号回复资料解压密码用于引流。所以， 我们在服务端处理用户消息时需要注意过滤，只有具备某些特征的消息才会去触发检索和LLM回复。

我们选用“@gpt”作为消息识别特征，只有消息以“@gpt”开头，我们才会对消息进行处理。

```python
def handle_auto_reply(self, msg):
        content = msg.content.strip().lower()
        if not content.startswith('@gpt'):
            return create_reply('', msg)
        rsp_content = self.create_gpt_res(content)
        return create_reply(rsp_content, msg)
```

在我们的项目中，对于非“@gpt”开头的消息，我们不做任何处理。

## 如何回复？

最直接的回复方式，当然是同步返回，即根据用户消息，检索知识库，调用LLM作答，最后再将结果返回给用户。

但对于我们使用的订阅号，不太适用，主要有以下原因：

1. 整个响应流程耗时一般会比较长，**但公众号要求业务逻辑处理时间不能超过 5 秒，否则会直接回复用户超时。**
2. 流式回复需要公众号的客服消息功能，但订阅号没有权限使用这个功能。这意味着只能等结果完全生成后才会返回给用户，这进一步加剧了超时的可能。
3. 长时间的不回复，不但会影响用户体验，还可能导致用户重复提问，造成服务重复地处理，浪费计算和调用资源。
4. 公众号对话框中回复消息，无法对消息进行排版，在某些情况下可能会显示很乱，比如回复的结果中包含代码；结果中有标题和正文区分等场景。

为了解决以上问题，我们采用一种“懒响应”的方式：对于用户的任何问题，我们都直接返回一个链接，用户点击该链接后，会跳到一个自定义页面，加载页面时我们再调用 LLM处理回答用户的问题。

继续上面`handle_auto_reply`代码的讲解，在确定消息以"@gpt"开头后，我们将消息内容传给`create_gpt_res`处理。以下是`create_gpt_res`方法的处理流程：

```python
def create_gpt_res(self, question):
        cache_key = str(uuid.uuid4())
        # 存储问题的key
        q_cache_key = cache_key + "#q"
        redis.set(q_cache_key, question)
        redis.expire(q_cache_key, 86400)
        # 存储答案的key, value留空
        res_cache_key = cache_key + "#res"
        redis.set(res_cache_key, "")
        redis.expire(res_cache_key, 86400)
        # 返回请求链接
        return '<a href="https://xxx.xxx.com/api/wechat/gpt?key={}">点击查看回复</a>'.format(cache_key)
```

> 注意代码中返回的路径是`/api/wechat/gpt`而非`/wechat/gpt`,这是因为部署时前面有个 nginx 转发，下面会详细介绍到。

首先，我们通过 uuid 生成一个随机 id，以该 id 为前缀创建两个 redis key：用于缓存问题的`q_cache_key`和用于缓存答案的`res_cache_key`。

然后，返回指向另外一个页面的a标签，生成的随机id会作为参数一并返回。

我们为`/wechat/gpt`路由创建了`WechatGptHandlerResource`资源类，`get`方法会返回一个响应结果展示页。

```python
class WechatGptHandlerResource(BaseResource):
    def get(self):
        return send_from_directory('static', 'index.html')
```

所以，当用户点击`create_gpt_res`返回的链接后，会调用`WechatGptHandlerResource::get`，返回一个静态页面。我们看看该页面的大概内容：

```html
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>会玩GPT</title>
    <script src="https://cdn.jsdelivr.net/npm/marked@3.0.8/marked.min.js"></script>
    <link href="https://cdn.bootcss.com/highlight.js/8.0/styles/monokai_sublime.min.css" rel="stylesheet">
    <script src="https://cdn.bootcss.com/highlight.js/8.0/highlight.min.js"></script>
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <style>
        ...
    </style>
</head>

<body>
    <div id="chat-container">让答案再飞一会～</div>
    <script>
        $(document).ready(function () {
            var url = window.location.href;
            console.log(url)
            var searchParams = new URLSearchParams(new URL(url).search);
            var queryKey = searchParams.get('key');
            console.log(queryKey)
            $.ajax({
                url: '/api/wechat/gpt',
                method: 'POST',
                data: {
                    key: queryKey,
                },
                success: function (response) {
                    var rendererMD = new marked.Renderer();
                    marked.setOptions({
                        renderer: rendererMD,
                        gfm: true,
                        tables: true,
                        breaks: false,
                        pedantic: false,
                        sanitize: false,
                        smartLists: true,
                        smartypants: false
                    });
                    marked.setOptions({
                        highlight: function (code) {
                            return hljs.highlightAuto(code).value;
                        }
                    });
                    const chatContainer = document.getElementById("chat-container");
                    chatContainer.innerHTML = ''
                    const messageElement = document.createElement("div");
                    messageElement.className = "message";
                    messageElement.innerHTML = window.marked(response.answer);
                    chatContainer.appendChild(messageElement);
                    // 添加引用信息
                    const referenceElement = document.createElement("div");
                    referenceElement.className = "reference";
                    response.metadatas.forEach(function (item) {
                        const referenceLink = document.createElement("a");
                        referenceLink.href = item.source;
                        referenceLink.textContent = item.title;
                        referenceLink.target = "_blank";
                        referenceElement.appendChild(referenceLink);
                        referenceElement.appendChild(document.createElement("br"));
                    });
                    chatContainer.appendChild(referenceElement);
                },
                error: function (error) {
                    console.log('API请求失败:', error);
                    const chatContainer = document.getElementById("chat-container");
                    chatContainer.innerHTML = "请求失败了，刷新页面试试"
                    // 处理API请求失败的逻辑
                }
            });
        })
    </script>
</body>
</html>
```

> 为了美化响应结果的排版，这里使用`marked.js` markdown 解析库将 Markdown 格式的结果文本解析并编译成 HTML。同时结合`highlight.js`，实现代码块的语法高亮展示。

在页面加载时，我们会以`POST`方式调用接口`/wechat/gpt`，并将上一步生成的随机 id 作为参数传递。

该调用最终会由`WechatGptHandlerResource::post`进行处理：

```python
class WechatGptHandlerResource(BaseResource):
    def post(self):
        cache_key = request.form.get("key", "")
        q_cache_key = cache_key + "#q"
        res_cache_key = cache_key + "#res"
        # 检查q是否存在redis
        if not redis.exists(q_cache_key):
            return make_response({"answer": "回答已过期"})
        question = redis.get(q_cache_key).decode('utf-8')
        current_res = redis.get(res_cache_key).decode('utf-8')
        if current_res == "":
            current_res = self.ask_gpt(question)
        redis.set(res_cache_key, current_res)
        return make_response(json.loads(current_res))
```

在每次调用 LLM 获取结果后，我们会将结果存入 redis 中(`res_cache_key`)，以后处理同一个链接的问题时，直接从 redis 中获取之前的答案即可，避免用户重复点击链接都会触发LLM 的调用。

`ask_gpt`就是检索知识库，调用 LLM 的代码逻辑。我们重点聊聊其调用`build_qa_chain`构建问答链的一些细节：

```python
def build_qa_chain(self):
        llm = ChatOpenAI(model_name="gpt-3.5-turbo-0125", openai_api_base=config.OPENAI_API_BASE, openai_api_key=config.OPENAI_API_KEY)
        vectorstore = Chroma(persist_directory=config.DB_STORE_DIR, embedding_function=get_embeddings_endpoint())
        
        # 构建提示词模版
        system_template = """Use the following pieces of context to answer the user's question.
        the answer are output in MARKDOWN format
        If you cannot get the answer from the context, just RESPOND: NO_ANSWER, don't try to make up an answer.
        ----------------
        context:
        {context}
        ----------------"""
        messages = [
            SystemMessagePromptTemplate.from_template(system_template),
            HumanMessagePromptTemplate.from_template("{question}"),
        ]
        chat_prompt = ChatPromptTemplate.from_messages(messages)
        
        return RetrievalQA.from_chain_type(
            llm,
            # 设定搜索结果数量并限制相似度阈值
            retriever=vectorstore.as_retriever(search_type ="similarity_score_threshold", search_kwargs={"k": 3, "score_threshold": 0.5}),
            return_source_documents=True,
            chain_type_kwargs={"prompt": chat_prompt}
        )
```

1. 为了适配`marked.js`，在提示词模板中，我们强调以 `markdown` 的格式返回结果。

2. 为了避免在知识库找不到相关性内容时仍然返回文档的错误结果，我们采用`similarity_score_threshold`的检索模式，并限制返回的文档与问题相关性必须高于 0.5（分数范围 0-1）。

3. `RetrievalQA`是 langchain 方便 RAG 应用开发设计的问答链构造类，其中`return_source_documents`表示返回检索到的文档集。

至此，整个项目的后端逻辑就介绍完成了。下面，我们看看如何部署我们的项目。

# 服务部署

## gunicorn多进程部署 + nginx 转发

`Flask` 虽然内置了一个简单的服务器，可以用来启动服务，但一般不会直接使用该内置服务器用于生产环境。

因为内置服务器是单线程的，这意味着它一次只能处理一个请求。比如，假如服务正在调用 LLM 等待响应，此时服务无法处理其他任何请求。

“专业的事情交给专业的人做”， 生产环境下，我们一般会选用专门的HTTP服务器来部署 python 项目。

这里，我们选用`gunicorn`来运行我们的服务。`gunicorn`在启动服务时，会创建多个工作进程，可以并行处理多个请求，提高应用的并发处理能力。

前端同学可以简单将`gunicorn`类比为 nodejs 中的`pm2`。

在项目的启动脚本中，有`gunicorn`的启动指令：

```shell
## bin/docker-entrypoint.sh
#!/bin/bash
set -e

server() {
  # Recycle gunicorn workers every n-th request. See http://docs.gunicorn.org/en/stable/settings.html#max-requests for more details.
  MAX_REQUESTS=${MAX_REQUESTS:-1000}
  MAX_REQUESTS_JITTER=${MAX_REQUESTS_JITTER:-100}
  exec pdm run gunicorn  --timeout 120 -b 0.0.0.0:5000 --name wx_bot -w${WXBOT_WORKERS:-4} wx_bot.wsgi:app --max-requests $MAX_REQUESTS --max-requests-jitter $MAX_REQUESTS_JITTER
}

case "$1" in
  server)
    shift
    server
    ;;
  *)
    exec "$@"
    ;;
esac
```

从上面可以看到，项目是监听到 5000 端口的。但我们对外服务是 443（https）端口，所以，在我们的服务前面，还有一层 nginx 转发。nginx 的配置如下：

```nginx
upstream wx_bot {
    server 127.0.0.1:5000;
}

server {
    listen 80 default_server;
    listen 443 ssl http2 default_server;
    server_name [替换成自己的域名];
    # ssl 相关配置
    ssl_certificate [替换成自己的证书];
    ssl_certificate_key  [替换成自己的私钥];
    ssl_protocols TLSv1.1 TLSv1.2 TLSv1.3;
    ssl_ciphers EECDH+CHACHA20:EECDH+CHACHA20-draft:EECDH+AES128:RSA+AES128:EECDH+AES256:RSA+AES256:EECDH+3DES:RSA+3DES:!MD5;
    ssl_prefer_server_ciphers on;
    ssl_session_cache shared:SSL:10m;
    ssl_session_timeout 10m;
    
    location /api/ {
        proxy_pass http://wx_bot/;
    }
}
```

上面的 nginx 配置可以看出，会将全部`/api` 前缀的请求转发到后端 wx\_bot服务中。比如`/api/wechat/gpt`请求最终会由 wx\_bot服务的`/wechat/gpt`路由进行处理。

这也解释了为什么`wx_bot/handlers/api.py`定义的路由都没有`/api`前缀，但在两个前端页面中请求的后端路由，都加了`/api`前缀。

## docker快速启动

为了避免服务器的环境异常导致项目启动失败，额外提供了服务对应的 docker 镜像，大家可以在自己服务器安装 docker 后，一键部署服务：

```shell
docker run -d -v [替换自己的 wx_bot/config目录]:/app/wx_bot/config -v [替换自己的向量数据库存储目录]:/app/db -p 5000:5000 mageelin/wx_bot:1.0.0
```

上面命令执行后，会监听到宿主机的 5000 端口，同样结合 nginx 对外服务。